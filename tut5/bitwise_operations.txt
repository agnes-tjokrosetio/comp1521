////////////////////////////////////////////////////////////////////////////////////////////////////

Assume that we have the following 16-bit variables defined and initialised:

    uint16_t a = 0x5555, b = 0xAAAA, c = 0x0001;

In binary:
    a = 0101 0101 0101 0101
    b = 1010 1010 1010 1010
    c = 0000 0000 0000 0001

////////////////////////////////////////////////////////////////////////////////////////////////////

-------------------

a | b = 1111 1111 1111 1111 = 0xffff

0101 0101 0101 0101
1010 1010 1010 1010
--------------------
1111 1111 1111 1111

-------------------

a & b = 0000 0000 0000 0000 = 0x0

0101 0101 0101 0101
1010 1010 1010 1010
--------------------
0000 0000 0000 0000

-------------------

a ^ b = 1111 1111 1111 1111 = 0xffff

0101 0101 0101 0101
1010 1010 1010 1010
--------------------
1111 1111 1111 1111

-------------------

a & ~b = 0101 0101 0101 0101 = '0x5555'

~b = ~(1010 1010 1010 1010) = 0101 0101 0101 0101

0101 0101 0101 0101
0101 0101 0101 0101
--------------------
0101 0101 0101 0101

-------------------

c << 6 = 0000 0000 0100 0000 = 0x40

0000 0000 0000 0001 << 6 = 0000 0000 0100 0000

-------------------
